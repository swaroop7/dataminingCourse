{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data from http://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\Downloads\\fitchburg\\courses\\datamining\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#data_folder = os.path.join(os.path.expanduser(\"ml-100k\\\\\")\n",
    "data_folder = os.path.join(os.path.expanduser(\"~\"), \"Downloads\\\\fitchburg\\\\courses\\\\datamining\\\\\")\n",
    "print data_folder\n",
    "ratings_filename = os.path.join(data_folder, \"BX-Book-Ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Favorable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating Favorable\n",
       "0   276725  034545104X            0     False\n",
       "1   276726  0155061224            5      True\n",
       "2   276727  0446520802            0     False\n",
       "3   276729  052165615X            3     False\n",
       "4   276729  0521795028            6      True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_ratings = pd.read_csv(ratings_filename, delimiter=\";\", header=None, names = [\"UserID\", \"MovieID\", \"Rating\", \"Datetime\"])\n",
    "all_ratings = pd.read_csv(ratings_filename, delimiter=\";\")\n",
    "#all_ratings[\"Datetime\"] = pd.to_datetime(all_ratings['Datetime'],unit='s')\n",
    "\n",
    "all_ratings = all_ratings[np.isfinite(all_ratings['Book-Rating'])]\n",
    "all_ratings['Book-Rating'] = all_ratings['Book-Rating'].apply(np.int64)\n",
    "\n",
    "all_ratings[\"Favorable\"] = all_ratings[\"Book-Rating\"] > 3\n",
    "\n",
    "all_ratings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278858, 3)\n",
      "(278858, 3)\n",
      "(1149780, 4)\n",
      "(1149780, 6)\n",
      "Index([u'User-ID', u'ISBN', u'Book-Rating', u'Favorable', u'Location', u'Age'], dtype='object')\n",
      "(1149780, 6)\n",
      "   User-ID        ISBN  Book-Rating Favorable                       Location  \\\n",
      "0   276725  034545104X            0     False              tyler, texas, usa   \n",
      "1   276726  0155061224            5      True       seattle, washington, usa   \n",
      "2   276727  0446520802            0     False  h, new south wales, australia   \n",
      "3   276729  052165615X            3     False           rijeka, n/a, croatia   \n",
      "4   276729  0521795028            6      True           rijeka, n/a, croatia   \n",
      "\n",
      "    Age  \n",
      "0   NaN  \n",
      "1   NaN  \n",
      "2  16.0  \n",
      "3  16.0  \n",
      "4  16.0  \n",
      "204801\n",
      "81263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:41: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107736\n",
      "               Favorable\n",
      "ISBN                    \n",
      " 9022906116          1.0\n",
      "0 14 02.9640         1.0\n",
      "0 7322 6794 3        0.0\n",
      "0 7336 1053 6        0.0\n",
      "0 75280 122 8        0.0\n",
      "There are 215 books with more than 15 favorable reviews\n",
      "I found 113 frequent itemsets of length 2\n",
      "I found 32 frequent itemsets of length 3\n",
      "I found 4 frequent itemsets of length 4\n",
      "Did not find any frequent itemsets of length 5\n"
     ]
    }
   ],
   "source": [
    "users_filename = os.path.join(data_folder, \"BX-Users.csv\")\n",
    "users = pd.read_csv(users_filename, delimiter=\";\")\n",
    "\n",
    "print users.shape\n",
    "\n",
    "users = users.dropna(subset= [\"Location\"])\n",
    "print users.shape\n",
    "\n",
    "result = pd.merge(all_ratings, users, how='inner', on=['User-ID'])\n",
    "\n",
    "print all_ratings.shape\n",
    "print result.shape\n",
    "\n",
    "print result.columns\n",
    "\n",
    "print result.shape\n",
    "\n",
    "print result[:5] \n",
    "\n",
    "\n",
    "# Sample the dataset. You can try increasing the size of the sample, but the run time will be considerably longer\n",
    "#ratings = all_ratings[all_ratings['User-ID'].isin(range(20000))]  # & \n",
    "ratings = result[result[\"User-ID\"].isin(range(50000))]\n",
    "#ratings = result.sample(frac=0.1, replace=False)\n",
    "#ratings = result\n",
    "print len(ratings)\n",
    "\n",
    "# We start by creating a dataset of each user's favourable reviews\n",
    "favorable_ratings = ratings[ratings[\"Favorable\"]]\n",
    "favorable_ratings[:5]\n",
    "\n",
    "print len(favorable_ratings)\n",
    "\n",
    "# We are only interested in the reviewers who have more than one review\n",
    "favorable_reviews_by_users = dict((k, frozenset(v.values)) for k, v in favorable_ratings.groupby(\"Location\")[\"ISBN\"])\n",
    "len(favorable_reviews_by_users)\n",
    "\n",
    "\n",
    "# Find out how many movies have favourable ratings\n",
    "num_favorable_by_movie = ratings[[\"ISBN\", \"Favorable\"]].groupby(\"ISBN\").sum()\n",
    "num_favorable_by_movie.sort(\"Favorable\", ascending=False)[:5]\n",
    "\n",
    "print len(num_favorable_by_movie)\n",
    "\n",
    "print num_favorable_by_movie[:5]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_frequent_itemsets(favorable_reviews_by_users, k_1_itemsets, min_support):\n",
    "    counts = defaultdict(int)\n",
    "    for user, reviews in favorable_reviews_by_users.items():\n",
    "        for itemset in k_1_itemsets:\n",
    "            if itemset.issubset(reviews):\n",
    "                for other_reviewed_movie in reviews - itemset:\n",
    "                    current_superset = itemset | frozenset((other_reviewed_movie,))\n",
    "                    counts[current_superset] += 1\n",
    "    return dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency >= min_support])\n",
    "\n",
    "\n",
    "import sys\n",
    "frequent_itemsets = {}  # itemsets are sorted by length\n",
    "min_support = 15\n",
    "\n",
    "# k=1 candidates are the isbns with more than min_support favourable reviews\n",
    "frequent_itemsets[1] = dict((frozenset((ISBN,)), row[\"Favorable\"])\n",
    "                                for ISBN, row in num_favorable_by_movie.iterrows()\n",
    "                                if row[\"Favorable\"] > min_support)\n",
    "\n",
    "print(\"There are {} books with more than {} favorable reviews\".format(len(frequent_itemsets[1]), min_support))\n",
    "sys.stdout.flush()\n",
    "for k in range(2, 20):\n",
    "    # Generate candidates of length k, using the frequent itemsets of length k-1\n",
    "    # Only store the frequent itemsets\n",
    "    cur_frequent_itemsets = find_frequent_itemsets(favorable_reviews_by_users, frequent_itemsets[k-1],\n",
    "                                                   min_support)\n",
    "    if len(cur_frequent_itemsets) == 0:\n",
    "        print(\"Did not find any frequent itemsets of length {}\".format(k))\n",
    "        sys.stdout.flush()\n",
    "        break\n",
    "    else:\n",
    "        print(\"I found {} frequent itemsets of length {}\".format(len(cur_frequent_itemsets), k))\n",
    "        #print(cur_frequent_itemsets)\n",
    "        sys.stdout.flush()\n",
    "        frequent_itemsets[k] = cur_frequent_itemsets\n",
    "# We aren't interested in the itemsets of length 1, so remove those\n",
    "del frequent_itemsets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 149 frequent itemsets\n",
      "There are 338 candidate rules\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(\"Found a total of {0} frequent itemsets\".format(sum(len(itemsets) for itemsets in frequent_itemsets.values())))\n",
    "\n",
    "# Now we create the association rules. First, they are candidates until the confidence has been tested\n",
    "candidate_rules = []\n",
    "for itemset_length, itemset_counts in frequent_itemsets.items():\n",
    "    for itemset in itemset_counts.keys():\n",
    "        for conclusion in itemset:\n",
    "            premise = itemset - set((conclusion,))\n",
    "            candidate_rules.append((premise, conclusion))\n",
    "print(\"There are {} candidate rules\".format(len(candidate_rules)))\n",
    "\n",
    "# Now, we compute the confidence of each of these rules. This is very similar to what we did in chapter 1\n",
    "correct_counts = defaultdict(int)\n",
    "incorrect_counts = defaultdict(int)\n",
    "for user, reviews in favorable_reviews_by_users.items():\n",
    "    for candidate_rule in candidate_rules:\n",
    "        premise, conclusion = candidate_rule\n",
    "        if premise.issubset(reviews):\n",
    "            if conclusion in reviews:\n",
    "                correct_counts[candidate_rule] += 1\n",
    "            else:\n",
    "                incorrect_counts[candidate_rule] += 1\n",
    "rule_confidence = {candidate_rule: correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule])\n",
    "              for candidate_rule in candidate_rules}\n",
    "\n",
    "\t\t\t  \n",
    "\t\t\t  \n",
    "# Choose only rules above a minimum confidence level\n",
    "min_confidence = 0.9\n",
    "\n",
    "# Filter out the rules with poor confidence\n",
    "rule_confidence = {rule: confidence for rule, confidence in rule_confidence.items() if confidence > min_confidence}\n",
    "print(len(rule_confidence))\n",
    "\n",
    "from operator import itemgetter\n",
    "sorted_confidence = sorted(rule_confidence.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271360, 8)\n",
      "Index([u'ISBN', u'Book-Title', u'Book-Author', u'Year-Of-Publication',\n",
      "       u'Publisher', u'Image-URL-S', u'Image-URL-M', u'Image-URL-L'],\n",
      "      dtype='object')\n",
      "Rule #1\n",
      "frozenset(['0590353403', '0439136350'])\n",
      "0439064864\n",
      "Rule: If a person recommends Harry Potter and the Sorcerer's Stone (Book 1), Harry Potter and the Prisoner of Azkaban (Book 3) they will also recommend Harry Potter and the Chamber of Secrets (Book 2)\n",
      " - Confidence: 1.000\n",
      "\n",
      "Rule #2\n",
      "frozenset(['0439139597', '0590353403'])\n",
      "0439064864\n",
      "Rule: If a person recommends Harry Potter and the Goblet of Fire (Book 4), Harry Potter and the Sorcerer's Stone (Book 1) they will also recommend Harry Potter and the Chamber of Secrets (Book 2)\n",
      " - Confidence: 0.909\n",
      "\n",
      "Rule #3\n",
      "frozenset(['0439064864', '0439136350'])\n",
      "0439139597\n",
      "Rule: If a person recommends Harry Potter and the Chamber of Secrets (Book 2), Harry Potter and the Prisoner of Azkaban (Book 3) they will also recommend Harry Potter and the Goblet of Fire (Book 4)\n",
      " - Confidence: 0.909\n",
      "\n",
      "Rule #4\n",
      "frozenset(['0439139597', '0439064864'])\n",
      "0439136350\n",
      "Rule: If a person recommends Harry Potter and the Goblet of Fire (Book 4), Harry Potter and the Chamber of Secrets (Book 2) they will also recommend Harry Potter and the Prisoner of Azkaban (Book 3)\n",
      " - Confidence: 0.909\n",
      "\n",
      "Rule #5\n",
      "frozenset(['0439139597', '0439064864'])\n",
      "0590353403\n",
      "Rule: If a person recommends Harry Potter and the Goblet of Fire (Book 4), Harry Potter and the Chamber of Secrets (Book 2) they will also recommend Harry Potter and the Sorcerer's Stone (Book 1)\n",
      " - Confidence: 0.909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_name_filename = os.path.join(data_folder, \"BX-Books.csv\")\n",
    "movie_name_data = pd.read_csv(movie_name_filename, delimiter=\";\", encoding = \"latin-1\", error_bad_lines = False, warn_bad_lines = False)\n",
    "#movie_name_data.columns = [\"MovieID\", \"Title\", \"Release Date\", \"Video Release\", \"IMDB\", \"<UNK>\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "print movie_name_data.shape\n",
    "print movie_name_data.columns\n",
    "\n",
    "def get_movie_name(movie_id):\n",
    "    title_object = movie_name_data[movie_name_data[\"ISBN\"] == str(movie_id)][\"Book-Title\"]\n",
    "    title = title_object.values[0]\n",
    "    return title\n",
    "\n",
    "count = 0\n",
    "while(count <5):  \n",
    "  count += 1\n",
    "  \n",
    "  try:  \n",
    "    print(\"Rule #{0}\".format(count))\n",
    "    (premise, conclusion) = sorted_confidence[count][0]\n",
    "    print premise\n",
    "    print conclusion\n",
    "    premise_names = \", \".join(get_movie_name(idx) for idx in premise)\n",
    "    conclusion_name = get_movie_name(str(conclusion))\n",
    "    print(\"Rule: If a person recommends {0} they will also recommend {1}\".format(premise_names, conclusion_name))\n",
    "    print(\" - Confidence: {0:.3f}\".format(rule_confidence[(premise, conclusion)]))\n",
    "    print(\"\")\n",
    "  except:\n",
    "    random_flag = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62584\n",
      "338\n",
      "[((frozenset(['0439139597', '0590353403', '0439136350']), '0439064864'), 1.0), ((frozenset(['0590353403', '0439136350']), '0439064864'), 1.0), ((frozenset(['0439064864', '0439136350']), '0439139597'), 0.9090909090909091), ((frozenset(['0439064864', '0439136350']), '0590353403'), 0.9090909090909091), ((frozenset(['0439139597', '0439064864']), '0590353403'), 0.9090909090909091)]\n",
      "Rule #1\n",
      "Rule: If a person recommends Harry Potter and the Goblet of Fire (Book 4), Harry Potter and the Sorcerer's Stone (Book 1), Harry Potter and the Prisoner of Azkaban (Book 3) they will also recommend Harry Potter and the Chamber of Secrets (Book 2)\n",
      " - Train Confidence: 1.000\n",
      " - Test Confidence: 1.000\n",
      "\n",
      "Rule #2\n",
      "Rule: If a person recommends Harry Potter and the Sorcerer's Stone (Book 1), Harry Potter and the Prisoner of Azkaban (Book 3) they will also recommend Harry Potter and the Chamber of Secrets (Book 2)\n",
      " - Train Confidence: 1.000\n",
      " - Test Confidence: 1.000\n",
      "\n",
      "Rule #3\n",
      "Rule: If a person recommends Harry Potter and the Goblet of Fire (Book 4), Harry Potter and the Sorcerer's Stone (Book 1) they will also recommend Harry Potter and the Chamber of Secrets (Book 2)\n",
      " - Train Confidence: 0.909\n",
      " - Test Confidence: 0.909\n",
      "\n",
      "Rule #4\n",
      "Rule: If a person recommends Harry Potter and the Chamber of Secrets (Book 2), Harry Potter and the Prisoner of Azkaban (Book 3) they will also recommend Harry Potter and the Goblet of Fire (Book 4)\n",
      " - Train Confidence: 0.909\n",
      " - Test Confidence: 0.909\n",
      "\n",
      "Rule #5\n",
      "Rule: If a person recommends Harry Potter and the Goblet of Fire (Book 4), Harry Potter and the Chamber of Secrets (Book 2) they will also recommend Harry Potter and the Prisoner of Azkaban (Book 3)\n",
      " - Train Confidence: 0.909\n",
      " - Test Confidence: 0.909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation using test data\n",
    "test_dataset = all_ratings[~all_ratings['User-ID'].isin(range(50000))]\n",
    "test_favorable = test_dataset[test_dataset[\"Favorable\"]]\n",
    "#test_not_favourable = test_dataset[~test_dataset[\"Favourable\"]]\n",
    "test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in test_favorable.groupby(\"User-ID\")[\"Book-Rating\"])\n",
    "#test_not_favourable_by_users = dict((k, frozenset(v.values)) for k, v in test_not_favourable.groupby(\"UserID\")[\"MovieID\"])\n",
    "#test_users = test_dataset[\"UserID\"].unique()\n",
    "\n",
    "print len(test_favorable_by_users)\n",
    "\n",
    "correct_counts = defaultdict(int)\n",
    "incorrect_counts = defaultdict(int)\n",
    "\n",
    "for user, reviews in test_favorable_by_users.items():\n",
    "    for candidate_rule in candidate_rules:\n",
    "        premise, conclusion = candidate_rule\n",
    "        if premise.issubset(reviews):\n",
    "            if conclusion in reviews:\n",
    "                correct_counts[candidate_rule] += 1\n",
    "            else:\n",
    "                incorrect_counts[candidate_rule] += 1\n",
    "\t\t\t\t\n",
    "# Now, we compute the confidence of each of these rules. This is very similar to what we did in chapter 1\n",
    "correct_counts = defaultdict(int)\n",
    "incorrect_counts = defaultdict(int)\n",
    "for user, reviews in favorable_reviews_by_users.items():\n",
    "    for candidate_rule in candidate_rules:\n",
    "        premise, conclusion = candidate_rule\n",
    "        if premise.issubset(reviews):\n",
    "            if conclusion in reviews:\n",
    "                correct_counts[candidate_rule] += 1\n",
    "            else:\n",
    "                incorrect_counts[candidate_rule] += 1\n",
    "rule_confidence = {candidate_rule: correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule])\n",
    "              for candidate_rule in candidate_rules}\n",
    "\n",
    "\t\t\t  \n",
    "test_confidence = {candidate_rule: correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule])\n",
    "                   for candidate_rule in rule_confidence}\n",
    "print(len(test_confidence))\n",
    "\n",
    "sorted_test_confidence = sorted(test_confidence.items(), key=itemgetter(1), reverse=True)\n",
    "print(sorted_test_confidence[:5])\n",
    "\n",
    "for index in range(5):\n",
    "    print(\"Rule #{0}\".format(index + 1))\n",
    "    (premise, conclusion) = sorted_confidence[index][0]\n",
    "    premise_names = \", \".join(get_movie_name(idx) for idx in premise)\n",
    "    conclusion_name = get_movie_name(conclusion)\n",
    "    print(\"Rule: If a person recommends {0} they will also recommend {1}\".format(premise_names, conclusion_name))\n",
    "    print(\" - Train Confidence: {0:.3f}\".format(rule_confidence.get((premise, conclusion), -1)))\n",
    "    print(\" - Test Confidence: {0:.3f}\".format(test_confidence.get((premise, conclusion), -1)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
